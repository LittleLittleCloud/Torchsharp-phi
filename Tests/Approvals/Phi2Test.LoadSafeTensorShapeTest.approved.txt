0: lm_head.bias: sum: -137.7653  dtype: BFloat16 shape: [51200]
1: lm_head.weight: sum: 7828.6465  dtype: BFloat16 shape: [51200,2560]
2: model.embed_tokens.weight: sum: 11265.593  dtype: BFloat16 shape: [51200,2560]
3: model.final_layernorm.bias: sum: 35.3614  dtype: BFloat16 shape: [2560]
4: model.final_layernorm.weight: sum: 32.7092  dtype: BFloat16 shape: [2560]
5: model.layers.0.input_layernorm.bias: sum: 37.2425  dtype: BFloat16 shape: [2560]
6: model.layers.0.input_layernorm.weight: sum: 26.5812  dtype: BFloat16 shape: [2560]
7: model.layers.0.mlp.fc1.bias: sum: 63.4317  dtype: BFloat16 shape: [10240]
8: model.layers.0.mlp.fc1.weight: sum: 3101.34  dtype: BFloat16 shape: [10240,2560]
9: model.layers.0.mlp.fc2.bias: sum: 16.8415  dtype: BFloat16 shape: [2560]
10: model.layers.0.mlp.fc2.weight: sum: 2390.321  dtype: BFloat16 shape: [2560,10240]
11: model.layers.0.self_attn.dense.bias: sum: 56.5626  dtype: BFloat16 shape: [2560]
12: model.layers.0.self_attn.dense.weight: sum: 1389.45  dtype: BFloat16 shape: [2560,2560]
13: model.layers.0.self_attn.k_proj.bias: sum: 56.1283  dtype: BFloat16 shape: [2560]
14: model.layers.0.self_attn.k_proj.weight: sum: 1784.2146  dtype: BFloat16 shape: [2560,2560]
15: model.layers.0.self_attn.q_proj.bias: sum: 193.2562  dtype: BFloat16 shape: [2560]
16: model.layers.0.self_attn.q_proj.weight: sum: 1879.1143  dtype: BFloat16 shape: [2560,2560]
17: model.layers.0.self_attn.v_proj.bias: sum: 22.4682  dtype: BFloat16 shape: [2560]
18: model.layers.0.self_attn.v_proj.weight: sum: 49256.992  dtype: BFloat16 shape: [2560,2560]
19: model.layers.1.input_layernorm.bias: sum: 30.2553  dtype: BFloat16 shape: [2560]
20: model.layers.1.input_layernorm.weight: sum: 39.0341  dtype: BFloat16 shape: [2560]
21: model.layers.1.mlp.fc1.bias: sum: 58.1005  dtype: BFloat16 shape: [10240]
22: model.layers.1.mlp.fc1.weight: sum: 2742.6343  dtype: BFloat16 shape: [10240,2560]
23: model.layers.1.mlp.fc2.bias: sum: 30.9621  dtype: BFloat16 shape: [2560]
24: model.layers.1.mlp.fc2.weight: sum: 2736.5598  dtype: BFloat16 shape: [2560,10240]
25: model.layers.1.self_attn.dense.bias: sum: 53.5885  dtype: BFloat16 shape: [2560]
26: model.layers.1.self_attn.dense.weight: sum: 1082.3087  dtype: BFloat16 shape: [2560,2560]
27: model.layers.1.self_attn.k_proj.bias: sum: 16.8774  dtype: BFloat16 shape: [2560]
28: model.layers.1.self_attn.k_proj.weight: sum: 880.3322  dtype: BFloat16 shape: [2560,2560]
29: model.layers.1.self_attn.q_proj.bias: sum: -81.955  dtype: BFloat16 shape: [2560]
30: model.layers.1.self_attn.q_proj.weight: sum: 1517.2854  dtype: BFloat16 shape: [2560,2560]
31: model.layers.1.self_attn.v_proj.bias: sum: 37.5824  dtype: BFloat16 shape: [2560]
32: model.layers.1.self_attn.v_proj.weight: sum: 2518.3333  dtype: BFloat16 shape: [2560,2560]
33: model.layers.10.input_layernorm.bias: sum: 27.7709  dtype: BFloat16 shape: [2560]
34: model.layers.10.input_layernorm.weight: sum: 34.1055  dtype: BFloat16 shape: [2560]
35: model.layers.10.mlp.fc1.bias: sum: 62.8471  dtype: BFloat16 shape: [10240]
36: model.layers.10.mlp.fc1.weight: sum: 2347.8025  dtype: BFloat16 shape: [10240,2560]
37: model.layers.10.mlp.fc2.bias: sum: 44.6007  dtype: BFloat16 shape: [2560]
38: model.layers.10.mlp.fc2.weight: sum: 3886.56  dtype: BFloat16 shape: [2560,10240]
39: model.layers.10.self_attn.dense.bias: sum: 36.949  dtype: BFloat16 shape: [2560]
40: model.layers.10.self_attn.dense.weight: sum: 1378.5819  dtype: BFloat16 shape: [2560,2560]
41: model.layers.10.self_attn.k_proj.bias: sum: 41.0794  dtype: BFloat16 shape: [2560]
42: model.layers.10.self_attn.k_proj.weight: sum: 2159.2476  dtype: BFloat16 shape: [2560,2560]
43: model.layers.10.self_attn.q_proj.bias: sum: 117.9559  dtype: BFloat16 shape: [2560]
44: model.layers.10.self_attn.q_proj.weight: sum: -4418.9717  dtype: BFloat16 shape: [2560,2560]
45: model.layers.10.self_attn.v_proj.bias: sum: 41.6369  dtype: BFloat16 shape: [2560]
46: model.layers.10.self_attn.v_proj.weight: sum: 839.5157  dtype: BFloat16 shape: [2560,2560]
47: model.layers.11.input_layernorm.bias: sum: 27.5384  dtype: BFloat16 shape: [2560]
48: model.layers.11.input_layernorm.weight: sum: 34.3209  dtype: BFloat16 shape: [2560]
49: model.layers.11.mlp.fc1.bias: sum: 0.5907  dtype: BFloat16 shape: [10240]
50: model.layers.11.mlp.fc1.weight: sum: 4312.6924  dtype: BFloat16 shape: [10240,2560]
51: model.layers.11.mlp.fc2.bias: sum: 42.2792  dtype: BFloat16 shape: [2560]
52: model.layers.11.mlp.fc2.weight: sum: 2616.57  dtype: BFloat16 shape: [2560,10240]
53: model.layers.11.self_attn.dense.bias: sum: 26.83  dtype: BFloat16 shape: [2560]
54: model.layers.11.self_attn.dense.weight: sum: 2560.8704  dtype: BFloat16 shape: [2560,2560]
55: model.layers.11.self_attn.k_proj.bias: sum: 13.542  dtype: BFloat16 shape: [2560]
56: model.layers.11.self_attn.k_proj.weight: sum: 1634.5759  dtype: BFloat16 shape: [2560,2560]
57: model.layers.11.self_attn.q_proj.bias: sum: 15.3241  dtype: BFloat16 shape: [2560]
58: model.layers.11.self_attn.q_proj.weight: sum: 1267.7246  dtype: BFloat16 shape: [2560,2560]
59: model.layers.11.self_attn.v_proj.bias: sum: 9.7873  dtype: BFloat16 shape: [2560]
60: model.layers.11.self_attn.v_proj.weight: sum: 1096.6749  dtype: BFloat16 shape: [2560,2560]
61: model.layers.12.input_layernorm.bias: sum: 30.5147  dtype: BFloat16 shape: [2560]
62: model.layers.12.input_layernorm.weight: sum: 34.6763  dtype: BFloat16 shape: [2560]
63: model.layers.12.mlp.fc1.bias: sum: 58.6804  dtype: BFloat16 shape: [10240]
64: model.layers.12.mlp.fc1.weight: sum: 2877.6587  dtype: BFloat16 shape: [10240,2560]
65: model.layers.12.mlp.fc2.bias: sum: 41.2488  dtype: BFloat16 shape: [2560]
66: model.layers.12.mlp.fc2.weight: sum: 3228.5864  dtype: BFloat16 shape: [2560,10240]
67: model.layers.12.self_attn.dense.bias: sum: 27.9659  dtype: BFloat16 shape: [2560]
68: model.layers.12.self_attn.dense.weight: sum: 2133.7537  dtype: BFloat16 shape: [2560,2560]
69: model.layers.12.self_attn.k_proj.bias: sum: 26.3767  dtype: BFloat16 shape: [2560]
70: model.layers.12.self_attn.k_proj.weight: sum: 1913.8588  dtype: BFloat16 shape: [2560,2560]
71: model.layers.12.self_attn.q_proj.bias: sum: 26.7829  dtype: BFloat16 shape: [2560]
72: model.layers.12.self_attn.q_proj.weight: sum: 2344.434  dtype: BFloat16 shape: [2560,2560]
73: model.layers.12.self_attn.v_proj.bias: sum: 20.2956  dtype: BFloat16 shape: [2560]
74: model.layers.12.self_attn.v_proj.weight: sum: 2150.198  dtype: BFloat16 shape: [2560,2560]
75: model.layers.13.input_layernorm.bias: sum: 25.4281  dtype: BFloat16 shape: [2560]
76: model.layers.13.input_layernorm.weight: sum: 34.6779  dtype: BFloat16 shape: [2560]
77: model.layers.13.mlp.fc1.bias: sum: 61.784  dtype: BFloat16 shape: [10240]
78: model.layers.13.mlp.fc1.weight: sum: 3918.646  dtype: BFloat16 shape: [10240,2560]
79: model.layers.13.mlp.fc2.bias: sum: 42.4784  dtype: BFloat16 shape: [2560]
80: model.layers.13.mlp.fc2.weight: sum: -697.9334  dtype: BFloat16 shape: [2560,10240]
81: model.layers.13.self_attn.dense.bias: sum: 27.0267  dtype: BFloat16 shape: [2560]
82: model.layers.13.self_attn.dense.weight: sum: 878.9269  dtype: BFloat16 shape: [2560,2560]
83: model.layers.13.self_attn.k_proj.bias: sum: 12.7306  dtype: BFloat16 shape: [2560]
84: model.layers.13.self_attn.k_proj.weight: sum: 751.0707  dtype: BFloat16 shape: [2560,2560]
85: model.layers.13.self_attn.q_proj.bias: sum: 31.1907  dtype: BFloat16 shape: [2560]
86: model.layers.13.self_attn.q_proj.weight: sum: 2266.2131  dtype: BFloat16 shape: [2560,2560]
87: model.layers.13.self_attn.v_proj.bias: sum: 47.3748  dtype: BFloat16 shape: [2560]
88: model.layers.13.self_attn.v_proj.weight: sum: 1139.4998  dtype: BFloat16 shape: [2560,2560]
89: model.layers.14.input_layernorm.bias: sum: 21.8274  dtype: BFloat16 shape: [2560]
90: model.layers.14.input_layernorm.weight: sum: 34.7881  dtype: BFloat16 shape: [2560]
91: model.layers.14.mlp.fc1.bias: sum: 62.3325  dtype: BFloat16 shape: [10240]
92: model.layers.14.mlp.fc1.weight: sum: 3813.1624  dtype: BFloat16 shape: [10240,2560]
93: model.layers.14.mlp.fc2.bias: sum: 51.2586  dtype: BFloat16 shape: [2560]
94: model.layers.14.mlp.fc2.weight: sum: 2008.8558  dtype: BFloat16 shape: [2560,10240]
95: model.layers.14.self_attn.dense.bias: sum: 26.1344  dtype: BFloat16 shape: [2560]
96: model.layers.14.self_attn.dense.weight: sum: 1759.7842  dtype: BFloat16 shape: [2560,2560]
97: model.layers.14.self_attn.k_proj.bias: sum: 40.3963  dtype: BFloat16 shape: [2560]
98: model.layers.14.self_attn.k_proj.weight: sum: 2134.6921  dtype: BFloat16 shape: [2560,2560]
99: model.layers.14.self_attn.q_proj.bias: sum: 40.3888  dtype: BFloat16 shape: [2560]
100: model.layers.14.self_attn.q_proj.weight: sum: 2020.859  dtype: BFloat16 shape: [2560,2560]
101: model.layers.14.self_attn.v_proj.bias: sum: -73.3771  dtype: BFloat16 shape: [2560]
102: model.layers.14.self_attn.v_proj.weight: sum: 2237.052  dtype: BFloat16 shape: [2560,2560]
103: model.layers.15.input_layernorm.bias: sum: 29.8866  dtype: BFloat16 shape: [2560]
104: model.layers.15.input_layernorm.weight: sum: 34.7416  dtype: BFloat16 shape: [2560]
105: model.layers.15.mlp.fc1.bias: sum: 74.7483  dtype: BFloat16 shape: [10240]
106: model.layers.15.mlp.fc1.weight: sum: 4023.108  dtype: BFloat16 shape: [10240,2560]
107: model.layers.15.mlp.fc2.bias: sum: 64.4063  dtype: BFloat16 shape: [2560]
108: model.layers.15.mlp.fc2.weight: sum: 2050.271  dtype: BFloat16 shape: [2560,10240]
109: model.layers.15.self_attn.dense.bias: sum: 26.5744  dtype: BFloat16 shape: [2560]
110: model.layers.15.self_attn.dense.weight: sum: 2366.1924  dtype: BFloat16 shape: [2560,2560]
111: model.layers.15.self_attn.k_proj.bias: sum: 17.459  dtype: BFloat16 shape: [2560]
112: model.layers.15.self_attn.k_proj.weight: sum: 3267.0554  dtype: BFloat16 shape: [2560,2560]
113: model.layers.15.self_attn.q_proj.bias: sum: 40.986  dtype: BFloat16 shape: [2560]
114: model.layers.15.self_attn.q_proj.weight: sum: 1333.745  dtype: BFloat16 shape: [2560,2560]
115: model.layers.15.self_attn.v_proj.bias: sum: 22.2151  dtype: BFloat16 shape: [2560]
116: model.layers.15.self_attn.v_proj.weight: sum: 508.9912  dtype: BFloat16 shape: [2560,2560]
117: model.layers.16.input_layernorm.bias: sum: 27.1968  dtype: BFloat16 shape: [2560]
118: model.layers.16.input_layernorm.weight: sum: 35.351  dtype: BFloat16 shape: [2560]
119: model.layers.16.mlp.fc1.bias: sum: 66.7514  dtype: BFloat16 shape: [10240]
120: model.layers.16.mlp.fc1.weight: sum: 3380.4932  dtype: BFloat16 shape: [10240,2560]
121: model.layers.16.mlp.fc2.bias: sum: -16.4443  dtype: BFloat16 shape: [2560]
122: model.layers.16.mlp.fc2.weight: sum: 1733.7072  dtype: BFloat16 shape: [2560,10240]
123: model.layers.16.self_attn.dense.bias: sum: 26.5619  dtype: BFloat16 shape: [2560]
124: model.layers.16.self_attn.dense.weight: sum: 564.1936  dtype: BFloat16 shape: [2560,2560]
125: model.layers.16.self_attn.k_proj.bias: sum: -26.4028  dtype: BFloat16 shape: [2560]
126: model.layers.16.self_attn.k_proj.weight: sum: 1292.0515  dtype: BFloat16 shape: [2560,2560]
127: model.layers.16.self_attn.q_proj.bias: sum: 41.489  dtype: BFloat16 shape: [2560]
128: model.layers.16.self_attn.q_proj.weight: sum: 190.3014  dtype: BFloat16 shape: [2560,2560]
129: model.layers.16.self_attn.v_proj.bias: sum: 36.1197  dtype: BFloat16 shape: [2560]
130: model.layers.16.self_attn.v_proj.weight: sum: 1059.2828  dtype: BFloat16 shape: [2560,2560]
131: model.layers.17.input_layernorm.bias: sum: 35.01  dtype: BFloat16 shape: [2560]
132: model.layers.17.input_layernorm.weight: sum: 35.011  dtype: BFloat16 shape: [2560]
133: model.layers.17.mlp.fc1.bias: sum: 150.682  dtype: BFloat16 shape: [10240]
134: model.layers.17.mlp.fc1.weight: sum: 3445.0757  dtype: BFloat16 shape: [10240,2560]
135: model.layers.17.mlp.fc2.bias: sum: 24.0245  dtype: BFloat16 shape: [2560]
136: model.layers.17.mlp.fc2.weight: sum: 2001.5344  dtype: BFloat16 shape: [2560,10240]
137: model.layers.17.self_attn.dense.bias: sum: 26.1403  dtype: BFloat16 shape: [2560]
138: model.layers.17.self_attn.dense.weight: sum: 522.1743  dtype: BFloat16 shape: [2560,2560]
139: model.layers.17.self_attn.k_proj.bias: sum: 24.7097  dtype: BFloat16 shape: [2560]
140: model.layers.17.self_attn.k_proj.weight: sum: 1436.8939  dtype: BFloat16 shape: [2560,2560]
141: model.layers.17.self_attn.q_proj.bias: sum: 18.5092  dtype: BFloat16 shape: [2560]
142: model.layers.17.self_attn.q_proj.weight: sum: 1374.792  dtype: BFloat16 shape: [2560,2560]
143: model.layers.17.self_attn.v_proj.bias: sum: 56.6659  dtype: BFloat16 shape: [2560]
144: model.layers.17.self_attn.v_proj.weight: sum: 1774.441  dtype: BFloat16 shape: [2560,2560]
145: model.layers.18.input_layernorm.bias: sum: 19.0293  dtype: BFloat16 shape: [2560]
146: model.layers.18.input_layernorm.weight: sum: 34.8967  dtype: BFloat16 shape: [2560]
147: model.layers.18.mlp.fc1.bias: sum: 59.2396  dtype: BFloat16 shape: [10240]
148: model.layers.18.mlp.fc1.weight: sum: 4418.935  dtype: BFloat16 shape: [10240,2560]
149: model.layers.18.mlp.fc2.bias: sum: 23.4659  dtype: BFloat16 shape: [2560]
150: model.layers.18.mlp.fc2.weight: sum: 2326.6484  dtype: BFloat16 shape: [2560,10240]
151: model.layers.18.self_attn.dense.bias: sum: 24.577  dtype: BFloat16 shape: [2560]
152: model.layers.18.self_attn.dense.weight: sum: 1166.0215  dtype: BFloat16 shape: [2560,2560]
153: model.layers.18.self_attn.k_proj.bias: sum: 42.4771  dtype: BFloat16 shape: [2560]
154: model.layers.18.self_attn.k_proj.weight: sum: 2283.8628  dtype: BFloat16 shape: [2560,2560]
155: model.layers.18.self_attn.q_proj.bias: sum: 23.6023  dtype: BFloat16 shape: [2560]
156: model.layers.18.self_attn.q_proj.weight: sum: 686.5198  dtype: BFloat16 shape: [2560,2560]
157: model.layers.18.self_attn.v_proj.bias: sum: 28.9251  dtype: BFloat16 shape: [2560]
158: model.layers.18.self_attn.v_proj.weight: sum: 594.8747  dtype: BFloat16 shape: [2560,2560]
159: model.layers.19.input_layernorm.bias: sum: 37.7922  dtype: BFloat16 shape: [2560]
160: model.layers.19.input_layernorm.weight: sum: 35.4239  dtype: BFloat16 shape: [2560]
161: model.layers.19.mlp.fc1.bias: sum: 122.0902  dtype: BFloat16 shape: [10240]
162: model.layers.19.mlp.fc1.weight: sum: 2976.5867  dtype: BFloat16 shape: [10240,2560]
163: model.layers.19.mlp.fc2.bias: sum: 19.3721  dtype: BFloat16 shape: [2560]
164: model.layers.19.mlp.fc2.weight: sum: 2389.0566  dtype: BFloat16 shape: [2560,10240]
165: model.layers.19.self_attn.dense.bias: sum: 26.4849  dtype: BFloat16 shape: [2560]
166: model.layers.19.self_attn.dense.weight: sum: 1951.9128  dtype: BFloat16 shape: [2560,2560]
167: model.layers.19.self_attn.k_proj.bias: sum: 42.3043  dtype: BFloat16 shape: [2560]
168: model.layers.19.self_attn.k_proj.weight: sum: -658.6146  dtype: BFloat16 shape: [2560,2560]
169: model.layers.19.self_attn.q_proj.bias: sum: 33.9043  dtype: BFloat16 shape: [2560]
170: model.layers.19.self_attn.q_proj.weight: sum: 2090.9524  dtype: BFloat16 shape: [2560,2560]
171: model.layers.19.self_attn.v_proj.bias: sum: 14.0905  dtype: BFloat16 shape: [2560]
172: model.layers.19.self_attn.v_proj.weight: sum: 1727.988  dtype: BFloat16 shape: [2560,2560]
173: model.layers.2.input_layernorm.bias: sum: 23.1563  dtype: BFloat16 shape: [2560]
174: model.layers.2.input_layernorm.weight: sum: 37.571  dtype: BFloat16 shape: [2560]
175: model.layers.2.mlp.fc1.bias: sum: 59.665  dtype: BFloat16 shape: [10240]
176: model.layers.2.mlp.fc1.weight: sum: 3227.9329  dtype: BFloat16 shape: [10240,2560]
177: model.layers.2.mlp.fc2.bias: sum: 94.9508  dtype: BFloat16 shape: [2560]
178: model.layers.2.mlp.fc2.weight: sum: 2757.4917  dtype: BFloat16 shape: [2560,10240]
179: model.layers.2.self_attn.dense.bias: sum: 49.6867  dtype: BFloat16 shape: [2560]
180: model.layers.2.self_attn.dense.weight: sum: 1091.0299  dtype: BFloat16 shape: [2560,2560]
181: model.layers.2.self_attn.k_proj.bias: sum: 12.6386  dtype: BFloat16 shape: [2560]
182: model.layers.2.self_attn.k_proj.weight: sum: 641.7961  dtype: BFloat16 shape: [2560,2560]
183: model.layers.2.self_attn.q_proj.bias: sum: 51.3105  dtype: BFloat16 shape: [2560]
184: model.layers.2.self_attn.q_proj.weight: sum: 1564.9769  dtype: BFloat16 shape: [2560,2560]
185: model.layers.2.self_attn.v_proj.bias: sum: 40.8567  dtype: BFloat16 shape: [2560]
186: model.layers.2.self_attn.v_proj.weight: sum: -1532.2017  dtype: BFloat16 shape: [2560,2560]
187: model.layers.20.input_layernorm.bias: sum: 18.9821  dtype: BFloat16 shape: [2560]
188: model.layers.20.input_layernorm.weight: sum: 34.6888  dtype: BFloat16 shape: [2560]
189: model.layers.20.mlp.fc1.bias: sum: 78.8865  dtype: BFloat16 shape: [10240]
190: model.layers.20.mlp.fc1.weight: sum: 3004.465  dtype: BFloat16 shape: [10240,2560]
191: model.layers.20.mlp.fc2.bias: sum: 26.656  dtype: BFloat16 shape: [2560]
192: model.layers.20.mlp.fc2.weight: sum: 2856.7014  dtype: BFloat16 shape: [2560,10240]
193: model.layers.20.self_attn.dense.bias: sum: 33.2205  dtype: BFloat16 shape: [2560]
194: model.layers.20.self_attn.dense.weight: sum: 1370.9874  dtype: BFloat16 shape: [2560,2560]
195: model.layers.20.self_attn.k_proj.bias: sum: 31.622  dtype: BFloat16 shape: [2560]
196: model.layers.20.self_attn.k_proj.weight: sum: 2081.6326  dtype: BFloat16 shape: [2560,2560]
197: model.layers.20.self_attn.q_proj.bias: sum: 42.6739  dtype: BFloat16 shape: [2560]
198: model.layers.20.self_attn.q_proj.weight: sum: 2130.4265  dtype: BFloat16 shape: [2560,2560]
199: model.layers.20.self_attn.v_proj.bias: sum: 88.7032  dtype: BFloat16 shape: [2560]
200: model.layers.20.self_attn.v_proj.weight: sum: 458.6672  dtype: BFloat16 shape: [2560,2560]
201: model.layers.21.input_layernorm.bias: sum: 49.8625  dtype: BFloat16 shape: [2560]
202: model.layers.21.input_layernorm.weight: sum: 35.4727  dtype: BFloat16 shape: [2560]
203: model.layers.21.mlp.fc1.bias: sum: 70.4009  dtype: BFloat16 shape: [10240]
204: model.layers.21.mlp.fc1.weight: sum: 3998.3127  dtype: BFloat16 shape: [10240,2560]
205: model.layers.21.mlp.fc2.bias: sum: 28.1991  dtype: BFloat16 shape: [2560]
206: model.layers.21.mlp.fc2.weight: sum: 2151.8  dtype: BFloat16 shape: [2560,10240]
207: model.layers.21.self_attn.dense.bias: sum: 23.5823  dtype: BFloat16 shape: [2560]
208: model.layers.21.self_attn.dense.weight: sum: 1071.3258  dtype: BFloat16 shape: [2560,2560]
209: model.layers.21.self_attn.k_proj.bias: sum: 42.3272  dtype: BFloat16 shape: [2560]
210: model.layers.21.self_attn.k_proj.weight: sum: 1192.4994  dtype: BFloat16 shape: [2560,2560]
211: model.layers.21.self_attn.q_proj.bias: sum: 27.5757  dtype: BFloat16 shape: [2560]
212: model.layers.21.self_attn.q_proj.weight: sum: 2717.5378  dtype: BFloat16 shape: [2560,2560]
213: model.layers.21.self_attn.v_proj.bias: sum: 40.5182  dtype: BFloat16 shape: [2560]
214: model.layers.21.self_attn.v_proj.weight: sum: 2382.1973  dtype: BFloat16 shape: [2560,2560]
215: model.layers.22.input_layernorm.bias: sum: 13.0777  dtype: BFloat16 shape: [2560]
216: model.layers.22.input_layernorm.weight: sum: 34.8764  dtype: BFloat16 shape: [2560]
217: model.layers.22.mlp.fc1.bias: sum: 70.2866  dtype: BFloat16 shape: [10240]
218: model.layers.22.mlp.fc1.weight: sum: 102.5399  dtype: BFloat16 shape: [10240,2560]
219: model.layers.22.mlp.fc2.bias: sum: 27.7158  dtype: BFloat16 shape: [2560]
220: model.layers.22.mlp.fc2.weight: sum: 2477.1233  dtype: BFloat16 shape: [2560,10240]
221: model.layers.22.self_attn.dense.bias: sum: 40.4925  dtype: BFloat16 shape: [2560]
222: model.layers.22.self_attn.dense.weight: sum: 5354.948  dtype: BFloat16 shape: [2560,2560]
223: model.layers.22.self_attn.k_proj.bias: sum: 41.9884  dtype: BFloat16 shape: [2560]
224: model.layers.22.self_attn.k_proj.weight: sum: 1298.5309  dtype: BFloat16 shape: [2560,2560]
225: model.layers.22.self_attn.q_proj.bias: sum: 43.1491  dtype: BFloat16 shape: [2560]
226: model.layers.22.self_attn.q_proj.weight: sum: 2354.4124  dtype: BFloat16 shape: [2560,2560]
227: model.layers.22.self_attn.v_proj.bias: sum: 35.0749  dtype: BFloat16 shape: [2560]
228: model.layers.22.self_attn.v_proj.weight: sum: 1694.7318  dtype: BFloat16 shape: [2560,2560]
229: model.layers.23.input_layernorm.bias: sum: 22.1077  dtype: BFloat16 shape: [2560]
230: model.layers.23.input_layernorm.weight: sum: 34.2226  dtype: BFloat16 shape: [2560]
231: model.layers.23.mlp.fc1.bias: sum: 33.8523  dtype: BFloat16 shape: [10240]
232: model.layers.23.mlp.fc1.weight: sum: 3293.0662  dtype: BFloat16 shape: [10240,2560]
233: model.layers.23.mlp.fc2.bias: sum: 32.8228  dtype: BFloat16 shape: [2560]
234: model.layers.23.mlp.fc2.weight: sum: 2653.578  dtype: BFloat16 shape: [2560,10240]
235: model.layers.23.self_attn.dense.bias: sum: 29.3725  dtype: BFloat16 shape: [2560]
236: model.layers.23.self_attn.dense.weight: sum: 1132.3827  dtype: BFloat16 shape: [2560,2560]
237: model.layers.23.self_attn.k_proj.bias: sum: 21.5598  dtype: BFloat16 shape: [2560]
238: model.layers.23.self_attn.k_proj.weight: sum: 1896.0112  dtype: BFloat16 shape: [2560,2560]
239: model.layers.23.self_attn.q_proj.bias: sum: 37.3012  dtype: BFloat16 shape: [2560]
240: model.layers.23.self_attn.q_proj.weight: sum: 2753.284  dtype: BFloat16 shape: [2560,2560]
241: model.layers.23.self_attn.v_proj.bias: sum: 38.4637  dtype: BFloat16 shape: [2560]
242: model.layers.23.self_attn.v_proj.weight: sum: 1745.1506  dtype: BFloat16 shape: [2560,2560]
243: model.layers.24.input_layernorm.bias: sum: 33.3483  dtype: BFloat16 shape: [2560]
244: model.layers.24.input_layernorm.weight: sum: 34.5287  dtype: BFloat16 shape: [2560]
245: model.layers.24.mlp.fc1.bias: sum: 456.4202  dtype: BFloat16 shape: [10240]
246: model.layers.24.mlp.fc1.weight: sum: 2274.017  dtype: BFloat16 shape: [10240,2560]
247: model.layers.24.mlp.fc2.bias: sum: 31.793  dtype: BFloat16 shape: [2560]
248: model.layers.24.mlp.fc2.weight: sum: 2643.5237  dtype: BFloat16 shape: [2560,10240]
249: model.layers.24.self_attn.dense.bias: sum: 33.3311  dtype: BFloat16 shape: [2560]
250: model.layers.24.self_attn.dense.weight: sum: 1404.4928  dtype: BFloat16 shape: [2560,2560]
251: model.layers.24.self_attn.k_proj.bias: sum: 11.863  dtype: BFloat16 shape: [2560]
252: model.layers.24.self_attn.k_proj.weight: sum: -739.8892  dtype: BFloat16 shape: [2560,2560]
253: model.layers.24.self_attn.q_proj.bias: sum: 8.7132  dtype: BFloat16 shape: [2560]
254: model.layers.24.self_attn.q_proj.weight: sum: 1687.3558  dtype: BFloat16 shape: [2560,2560]
255: model.layers.24.self_attn.v_proj.bias: sum: 66.2737  dtype: BFloat16 shape: [2560]
256: model.layers.24.self_attn.v_proj.weight: sum: 733.0276  dtype: BFloat16 shape: [2560,2560]
257: model.layers.25.input_layernorm.bias: sum: 20.4264  dtype: BFloat16 shape: [2560]
258: model.layers.25.input_layernorm.weight: sum: 34.0938  dtype: BFloat16 shape: [2560]
259: model.layers.25.mlp.fc1.bias: sum: 30.2369  dtype: BFloat16 shape: [10240]
260: model.layers.25.mlp.fc1.weight: sum: -584.566  dtype: BFloat16 shape: [10240,2560]
261: model.layers.25.mlp.fc2.bias: sum: 33.0566  dtype: BFloat16 shape: [2560]
262: model.layers.25.mlp.fc2.weight: sum: 2839.1343  dtype: BFloat16 shape: [2560,10240]
263: model.layers.25.self_attn.dense.bias: sum: 31.904  dtype: BFloat16 shape: [2560]
264: model.layers.25.self_attn.dense.weight: sum: 1469.3832  dtype: BFloat16 shape: [2560,2560]
265: model.layers.25.self_attn.k_proj.bias: sum: 7.7757  dtype: BFloat16 shape: [2560]
266: model.layers.25.self_attn.k_proj.weight: sum: 1969.6302  dtype: BFloat16 shape: [2560,2560]
267: model.layers.25.self_attn.q_proj.bias: sum: 7.7691  dtype: BFloat16 shape: [2560]
268: model.layers.25.self_attn.q_proj.weight: sum: 472.9609  dtype: BFloat16 shape: [2560,2560]
269: model.layers.25.self_attn.v_proj.bias: sum: 79.191  dtype: BFloat16 shape: [2560]
270: model.layers.25.self_attn.v_proj.weight: sum: 582.4239  dtype: BFloat16 shape: [2560,2560]
271: model.layers.26.input_layernorm.bias: sum: 17.2921  dtype: BFloat16 shape: [2560]
272: model.layers.26.input_layernorm.weight: sum: 33.977  dtype: BFloat16 shape: [2560]
273: model.layers.26.mlp.fc1.bias: sum: 60.0688  dtype: BFloat16 shape: [10240]
274: model.layers.26.mlp.fc1.weight: sum: 3020.342  dtype: BFloat16 shape: [10240,2560]
275: model.layers.26.mlp.fc2.bias: sum: 33.4152  dtype: BFloat16 shape: [2560]
276: model.layers.26.mlp.fc2.weight: sum: 3120.7805  dtype: BFloat16 shape: [2560,10240]
277: model.layers.26.self_attn.dense.bias: sum: 35.2157  dtype: BFloat16 shape: [2560]
278: model.layers.26.self_attn.dense.weight: sum: 2244.0059  dtype: BFloat16 shape: [2560,2560]
279: model.layers.26.self_attn.k_proj.bias: sum: 34.7205  dtype: BFloat16 shape: [2560]
280: model.layers.26.self_attn.k_proj.weight: sum: 654.8708  dtype: BFloat16 shape: [2560,2560]
281: model.layers.26.self_attn.q_proj.bias: sum: 31.2659  dtype: BFloat16 shape: [2560]
282: model.layers.26.self_attn.q_proj.weight: sum: 1465.3639  dtype: BFloat16 shape: [2560,2560]
283: model.layers.26.self_attn.v_proj.bias: sum: 26.6989  dtype: BFloat16 shape: [2560]
284: model.layers.26.self_attn.v_proj.weight: sum: 1491.4164  dtype: BFloat16 shape: [2560,2560]
285: model.layers.27.input_layernorm.bias: sum: 19.1887  dtype: BFloat16 shape: [2560]
286: model.layers.27.input_layernorm.weight: sum: 33.5065  dtype: BFloat16 shape: [2560]
287: model.layers.27.mlp.fc1.bias: sum: 52.5022  dtype: BFloat16 shape: [10240]
288: model.layers.27.mlp.fc1.weight: sum: 3866.6309  dtype: BFloat16 shape: [10240,2560]
289: model.layers.27.mlp.fc2.bias: sum: 32.5934  dtype: BFloat16 shape: [2560]
290: model.layers.27.mlp.fc2.weight: sum: 2494.021  dtype: BFloat16 shape: [2560,10240]
291: model.layers.27.self_attn.dense.bias: sum: 82  dtype: BFloat16 shape: [2560]
292: model.layers.27.self_attn.dense.weight: sum: 2420.064  dtype: BFloat16 shape: [2560,2560]
293: model.layers.27.self_attn.k_proj.bias: sum: 26.7792  dtype: BFloat16 shape: [2560]
294: model.layers.27.self_attn.k_proj.weight: sum: 1326.8619  dtype: BFloat16 shape: [2560,2560]
295: model.layers.27.self_attn.q_proj.bias: sum: 13.0033  dtype: BFloat16 shape: [2560]
296: model.layers.27.self_attn.q_proj.weight: sum: 829.4328  dtype: BFloat16 shape: [2560,2560]
297: model.layers.27.self_attn.v_proj.bias: sum: 9.1059  dtype: BFloat16 shape: [2560]
298: model.layers.27.self_attn.v_proj.weight: sum: 2401.8118  dtype: BFloat16 shape: [2560,2560]
299: model.layers.28.input_layernorm.bias: sum: 36.2559  dtype: BFloat16 shape: [2560]
300: model.layers.28.input_layernorm.weight: sum: 33.3647  dtype: BFloat16 shape: [2560]
301: model.layers.28.mlp.fc1.bias: sum: 88.4098  dtype: BFloat16 shape: [10240]
302: model.layers.28.mlp.fc1.weight: sum: 4249.1724  dtype: BFloat16 shape: [10240,2560]
303: model.layers.28.mlp.fc2.bias: sum: 46.3018  dtype: BFloat16 shape: [2560]
304: model.layers.28.mlp.fc2.weight: sum: 1173.2174  dtype: BFloat16 shape: [2560,10240]
305: model.layers.28.self_attn.dense.bias: sum: 86.2368  dtype: BFloat16 shape: [2560]
306: model.layers.28.self_attn.dense.weight: sum: 954.683  dtype: BFloat16 shape: [2560,2560]
307: model.layers.28.self_attn.k_proj.bias: sum: 22.9689  dtype: BFloat16 shape: [2560]
308: model.layers.28.self_attn.k_proj.weight: sum: 702.4904  dtype: BFloat16 shape: [2560,2560]
309: model.layers.28.self_attn.q_proj.bias: sum: 18.9184  dtype: BFloat16 shape: [2560]
310: model.layers.28.self_attn.q_proj.weight: sum: 3497.9077  dtype: BFloat16 shape: [2560,2560]
311: model.layers.28.self_attn.v_proj.bias: sum: 29.7626  dtype: BFloat16 shape: [2560]
312: model.layers.28.self_attn.v_proj.weight: sum: -1560.8134  dtype: BFloat16 shape: [2560,2560]
313: model.layers.29.input_layernorm.bias: sum: 27.9176  dtype: BFloat16 shape: [2560]
314: model.layers.29.input_layernorm.weight: sum: 48.1702  dtype: BFloat16 shape: [2560]
315: model.layers.29.mlp.fc1.bias: sum: 88.6355  dtype: BFloat16 shape: [10240]
316: model.layers.29.mlp.fc1.weight: sum: 4364.357  dtype: BFloat16 shape: [10240,2560]
317: model.layers.29.mlp.fc2.bias: sum: 18.3088  dtype: BFloat16 shape: [2560]
318: model.layers.29.mlp.fc2.weight: sum: 2757.681  dtype: BFloat16 shape: [2560,10240]
319: model.layers.29.self_attn.dense.bias: sum: 27.3034  dtype: BFloat16 shape: [2560]
320: model.layers.29.self_attn.dense.weight: sum: 2227.6748  dtype: BFloat16 shape: [2560,2560]
321: model.layers.29.self_attn.k_proj.bias: sum: 12.1413  dtype: BFloat16 shape: [2560]
322: model.layers.29.self_attn.k_proj.weight: sum: 4558.0986  dtype: BFloat16 shape: [2560,2560]
323: model.layers.29.self_attn.q_proj.bias: sum: 18.5449  dtype: BFloat16 shape: [2560]
324: model.layers.29.self_attn.q_proj.weight: sum: -605.779  dtype: BFloat16 shape: [2560,2560]
325: model.layers.29.self_attn.v_proj.bias: sum: 11.0765  dtype: BFloat16 shape: [2560]
326: model.layers.29.self_attn.v_proj.weight: sum: -3126.1667  dtype: BFloat16 shape: [2560,2560]
327: model.layers.3.input_layernorm.bias: sum: 27.232  dtype: BFloat16 shape: [2560]
328: model.layers.3.input_layernorm.weight: sum: 35.6045  dtype: BFloat16 shape: [2560]
329: model.layers.3.mlp.fc1.bias: sum: 77.3077  dtype: BFloat16 shape: [10240]
330: model.layers.3.mlp.fc1.weight: sum: 4240.0884  dtype: BFloat16 shape: [10240,2560]
331: model.layers.3.mlp.fc2.bias: sum: 8.2308  dtype: BFloat16 shape: [2560]
332: model.layers.3.mlp.fc2.weight: sum: 2762.8164  dtype: BFloat16 shape: [2560,10240]
333: model.layers.3.self_attn.dense.bias: sum: 44.3907  dtype: BFloat16 shape: [2560]
334: model.layers.3.self_attn.dense.weight: sum: 1381.4576  dtype: BFloat16 shape: [2560,2560]
335: model.layers.3.self_attn.k_proj.bias: sum: 33.1969  dtype: BFloat16 shape: [2560]
336: model.layers.3.self_attn.k_proj.weight: sum: -5315.2  dtype: BFloat16 shape: [2560,2560]
337: model.layers.3.self_attn.q_proj.bias: sum: -28.3854  dtype: BFloat16 shape: [2560]
338: model.layers.3.self_attn.q_proj.weight: sum: 1187.6061  dtype: BFloat16 shape: [2560,2560]
339: model.layers.3.self_attn.v_proj.bias: sum: 40.8001  dtype: BFloat16 shape: [2560]
340: model.layers.3.self_attn.v_proj.weight: sum: 1554.8005  dtype: BFloat16 shape: [2560,2560]
341: model.layers.30.input_layernorm.bias: sum: 24.7508  dtype: BFloat16 shape: [2560]
342: model.layers.30.input_layernorm.weight: sum: 47.0105  dtype: BFloat16 shape: [2560]
343: model.layers.30.mlp.fc1.bias: sum: 31.496  dtype: BFloat16 shape: [10240]
344: model.layers.30.mlp.fc1.weight: sum: 4739.3403  dtype: BFloat16 shape: [10240,2560]
345: model.layers.30.mlp.fc2.bias: sum: 30.1237  dtype: BFloat16 shape: [2560]
346: model.layers.30.mlp.fc2.weight: sum: 2727.0916  dtype: BFloat16 shape: [2560,10240]
347: model.layers.30.self_attn.dense.bias: sum: 24.8399  dtype: BFloat16 shape: [2560]
348: model.layers.30.self_attn.dense.weight: sum: -1390.2914  dtype: BFloat16 shape: [2560,2560]
349: model.layers.30.self_attn.k_proj.bias: sum: 7.2789  dtype: BFloat16 shape: [2560]
350: model.layers.30.self_attn.k_proj.weight: sum: 2418.5957  dtype: BFloat16 shape: [2560,2560]
351: model.layers.30.self_attn.q_proj.bias: sum: 40.4372  dtype: BFloat16 shape: [2560]
352: model.layers.30.self_attn.q_proj.weight: sum: 1215.7574  dtype: BFloat16 shape: [2560,2560]
353: model.layers.30.self_attn.v_proj.bias: sum: 36.9954  dtype: BFloat16 shape: [2560]
354: model.layers.30.self_attn.v_proj.weight: sum: 1866.1425  dtype: BFloat16 shape: [2560,2560]
355: model.layers.31.input_layernorm.bias: sum: 21.5556  dtype: BFloat16 shape: [2560]
356: model.layers.31.input_layernorm.weight: sum: 48.2541  dtype: BFloat16 shape: [2560]
357: model.layers.31.mlp.fc1.bias: sum: 80.0767  dtype: BFloat16 shape: [10240]
358: model.layers.31.mlp.fc1.weight: sum: 4309.4336  dtype: BFloat16 shape: [10240,2560]
359: model.layers.31.mlp.fc2.bias: sum: 27.2213  dtype: BFloat16 shape: [2560]
360: model.layers.31.mlp.fc2.weight: sum: 2829.6506  dtype: BFloat16 shape: [2560,10240]
361: model.layers.31.self_attn.dense.bias: sum: 27.4466  dtype: BFloat16 shape: [2560]
362: model.layers.31.self_attn.dense.weight: sum: 1355.5331  dtype: BFloat16 shape: [2560,2560]
363: model.layers.31.self_attn.k_proj.bias: sum: 26.4188  dtype: BFloat16 shape: [2560]
364: model.layers.31.self_attn.k_proj.weight: sum: 837.8412  dtype: BFloat16 shape: [2560,2560]
365: model.layers.31.self_attn.q_proj.bias: sum: 31.6047  dtype: BFloat16 shape: [2560]
366: model.layers.31.self_attn.q_proj.weight: sum: 989.1101  dtype: BFloat16 shape: [2560,2560]
367: model.layers.31.self_attn.v_proj.bias: sum: 8.1239  dtype: BFloat16 shape: [2560]
368: model.layers.31.self_attn.v_proj.weight: sum: 410.2112  dtype: BFloat16 shape: [2560,2560]
369: model.layers.4.input_layernorm.bias: sum: 27.7497  dtype: BFloat16 shape: [2560]
370: model.layers.4.input_layernorm.weight: sum: 35.1581  dtype: BFloat16 shape: [2560]
371: model.layers.4.mlp.fc1.bias: sum: 61.319  dtype: BFloat16 shape: [10240]
372: model.layers.4.mlp.fc1.weight: sum: 3700.122  dtype: BFloat16 shape: [10240,2560]
373: model.layers.4.mlp.fc2.bias: sum: 11.6841  dtype: BFloat16 shape: [2560]
374: model.layers.4.mlp.fc2.weight: sum: 2760.6665  dtype: BFloat16 shape: [2560,10240]
375: model.layers.4.self_attn.dense.bias: sum: 51.8143  dtype: BFloat16 shape: [2560]
376: model.layers.4.self_attn.dense.weight: sum: 1384.3285  dtype: BFloat16 shape: [2560,2560]
377: model.layers.4.self_attn.k_proj.bias: sum: 34.9312  dtype: BFloat16 shape: [2560]
378: model.layers.4.self_attn.k_proj.weight: sum: 1174.6962  dtype: BFloat16 shape: [2560,2560]
379: model.layers.4.self_attn.q_proj.bias: sum: 32.9853  dtype: BFloat16 shape: [2560]
380: model.layers.4.self_attn.q_proj.weight: sum: 2555.09  dtype: BFloat16 shape: [2560,2560]
381: model.layers.4.self_attn.v_proj.bias: sum: 44.6114  dtype: BFloat16 shape: [2560]
382: model.layers.4.self_attn.v_proj.weight: sum: 1130.6818  dtype: BFloat16 shape: [2560,2560]
383: model.layers.5.input_layernorm.bias: sum: 26.0325  dtype: BFloat16 shape: [2560]
384: model.layers.5.input_layernorm.weight: sum: 34.5158  dtype: BFloat16 shape: [2560]
385: model.layers.5.mlp.fc1.bias: sum: 44.0521  dtype: BFloat16 shape: [10240]
386: model.layers.5.mlp.fc1.weight: sum: 2201.491  dtype: BFloat16 shape: [10240,2560]
387: model.layers.5.mlp.fc2.bias: sum: 48.8656  dtype: BFloat16 shape: [2560]
388: model.layers.5.mlp.fc2.weight: sum: 2706.5862  dtype: BFloat16 shape: [2560,10240]
389: model.layers.5.self_attn.dense.bias: sum: 27.469  dtype: BFloat16 shape: [2560]
390: model.layers.5.self_attn.dense.weight: sum: 1312.531  dtype: BFloat16 shape: [2560,2560]
391: model.layers.5.self_attn.k_proj.bias: sum: 24.7459  dtype: BFloat16 shape: [2560]
392: model.layers.5.self_attn.k_proj.weight: sum: 1101.5254  dtype: BFloat16 shape: [2560,2560]
393: model.layers.5.self_attn.q_proj.bias: sum: 32.1523  dtype: BFloat16 shape: [2560]
394: model.layers.5.self_attn.q_proj.weight: sum: -2510.849  dtype: BFloat16 shape: [2560,2560]
395: model.layers.5.self_attn.v_proj.bias: sum: 47.9612  dtype: BFloat16 shape: [2560]
396: model.layers.5.self_attn.v_proj.weight: sum: 1812.556  dtype: BFloat16 shape: [2560,2560]
397: model.layers.6.input_layernorm.bias: sum: 27.738  dtype: BFloat16 shape: [2560]
398: model.layers.6.input_layernorm.weight: sum: 34.3084  dtype: BFloat16 shape: [2560]
399: model.layers.6.mlp.fc1.bias: sum: 44.0343  dtype: BFloat16 shape: [10240]
400: model.layers.6.mlp.fc1.weight: sum: 2750.6606  dtype: BFloat16 shape: [10240,2560]
401: model.layers.6.mlp.fc2.bias: sum: 48.6343  dtype: BFloat16 shape: [2560]
402: model.layers.6.mlp.fc2.weight: sum: 2748.8716  dtype: BFloat16 shape: [2560,10240]
403: model.layers.6.self_attn.dense.bias: sum: 21.6358  dtype: BFloat16 shape: [2560]
404: model.layers.6.self_attn.dense.weight: sum: 1893.3104  dtype: BFloat16 shape: [2560,2560]
405: model.layers.6.self_attn.k_proj.bias: sum: 37.4164  dtype: BFloat16 shape: [2560]
406: model.layers.6.self_attn.k_proj.weight: sum: 2382.9573  dtype: BFloat16 shape: [2560,2560]
407: model.layers.6.self_attn.q_proj.bias: sum: 31.5736  dtype: BFloat16 shape: [2560]
408: model.layers.6.self_attn.q_proj.weight: sum: -88.3149  dtype: BFloat16 shape: [2560,2560]
409: model.layers.6.self_attn.v_proj.bias: sum: 40.623  dtype: BFloat16 shape: [2560]
410: model.layers.6.self_attn.v_proj.weight: sum: 1177.0092  dtype: BFloat16 shape: [2560,2560]
411: model.layers.7.input_layernorm.bias: sum: 22.2724  dtype: BFloat16 shape: [2560]
412: model.layers.7.input_layernorm.weight: sum: 33.9238  dtype: BFloat16 shape: [2560]
413: model.layers.7.mlp.fc1.bias: sum: 23.2376  dtype: BFloat16 shape: [10240]
414: model.layers.7.mlp.fc1.weight: sum: 4214.03  dtype: BFloat16 shape: [10240,2560]
415: model.layers.7.mlp.fc2.bias: sum: 42.9558  dtype: BFloat16 shape: [2560]
416: model.layers.7.mlp.fc2.weight: sum: 2576.723  dtype: BFloat16 shape: [2560,10240]
417: model.layers.7.self_attn.dense.bias: sum: 34.1464  dtype: BFloat16 shape: [2560]
418: model.layers.7.self_attn.dense.weight: sum: -355.4494  dtype: BFloat16 shape: [2560,2560]
419: model.layers.7.self_attn.k_proj.bias: sum: 25.0185  dtype: BFloat16 shape: [2560]
420: model.layers.7.self_attn.k_proj.weight: sum: 1747.7885  dtype: BFloat16 shape: [2560,2560]
421: model.layers.7.self_attn.q_proj.bias: sum: -157.7367  dtype: BFloat16 shape: [2560]
422: model.layers.7.self_attn.q_proj.weight: sum: -1007.8965  dtype: BFloat16 shape: [2560,2560]
423: model.layers.7.self_attn.v_proj.bias: sum: 24.9293  dtype: BFloat16 shape: [2560]
424: model.layers.7.self_attn.v_proj.weight: sum: 1233.232  dtype: BFloat16 shape: [2560,2560]
425: model.layers.8.input_layernorm.bias: sum: 28.5647  dtype: BFloat16 shape: [2560]
426: model.layers.8.input_layernorm.weight: sum: 34.264  dtype: BFloat16 shape: [2560]
427: model.layers.8.mlp.fc1.bias: sum: 83.9514  dtype: BFloat16 shape: [10240]
428: model.layers.8.mlp.fc1.weight: sum: 2873.825  dtype: BFloat16 shape: [10240,2560]
429: model.layers.8.mlp.fc2.bias: sum: 42.4459  dtype: BFloat16 shape: [2560]
430: model.layers.8.mlp.fc2.weight: sum: 2137.9617  dtype: BFloat16 shape: [2560,10240]
431: model.layers.8.self_attn.dense.bias: sum: 32.7413  dtype: BFloat16 shape: [2560]
432: model.layers.8.self_attn.dense.weight: sum: 22945.729  dtype: BFloat16 shape: [2560,2560]
433: model.layers.8.self_attn.k_proj.bias: sum: 35.1717  dtype: BFloat16 shape: [2560]
434: model.layers.8.self_attn.k_proj.weight: sum: 6745.5425  dtype: BFloat16 shape: [2560,2560]
435: model.layers.8.self_attn.q_proj.bias: sum: 43.3795  dtype: BFloat16 shape: [2560]
436: model.layers.8.self_attn.q_proj.weight: sum: -429.8014  dtype: BFloat16 shape: [2560,2560]
437: model.layers.8.self_attn.v_proj.bias: sum: 35.4066  dtype: BFloat16 shape: [2560]
438: model.layers.8.self_attn.v_proj.weight: sum: 2102.4495  dtype: BFloat16 shape: [2560,2560]
439: model.layers.9.input_layernorm.bias: sum: 29.0312  dtype: BFloat16 shape: [2560]
440: model.layers.9.input_layernorm.weight: sum: 34.1209  dtype: BFloat16 shape: [2560]
441: model.layers.9.mlp.fc1.bias: sum: 72.1972  dtype: BFloat16 shape: [10240]
442: model.layers.9.mlp.fc1.weight: sum: 2426.8052  dtype: BFloat16 shape: [10240,2560]
443: model.layers.9.mlp.fc2.bias: sum: 45.555  dtype: BFloat16 shape: [2560]
444: model.layers.9.mlp.fc2.weight: sum: 2707.5955  dtype: BFloat16 shape: [2560,10240]
445: model.layers.9.self_attn.dense.bias: sum: 35.2898  dtype: BFloat16 shape: [2560]
446: model.layers.9.self_attn.dense.weight: sum: 1282.4036  dtype: BFloat16 shape: [2560,2560]
447: model.layers.9.self_attn.k_proj.bias: sum: 26.4502  dtype: BFloat16 shape: [2560]
448: model.layers.9.self_attn.k_proj.weight: sum: 1971.5452  dtype: BFloat16 shape: [2560,2560]
449: model.layers.9.self_attn.q_proj.bias: sum: 48.9294  dtype: BFloat16 shape: [2560]
450: model.layers.9.self_attn.q_proj.weight: sum: 1901.6754  dtype: BFloat16 shape: [2560,2560]
451: model.layers.9.self_attn.v_proj.bias: sum: 5.9886  dtype: BFloat16 shape: [2560]
452: model.layers.9.self_attn.v_proj.weight: sum: 2600.5972  dtype: BFloat16 shape: [2560,2560]

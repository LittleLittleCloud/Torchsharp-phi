0: lm_head.weight: sum: 7113.624  dtype: BFloat16 shape: [32064,3072]
1: model.embed_tokens.weight: sum: 7050.145  dtype: BFloat16 shape: [32064,3072]
2: model.layers.0.input_layernorm.weight: sum: 36.8088  dtype: BFloat16 shape: [3072]
3: model.layers.0.mlp.down_proj.weight: sum: 2950.9963  dtype: BFloat16 shape: [3072,8192]
4: model.layers.0.mlp.gate_up_proj.weight: sum: 4084.5117  dtype: BFloat16 shape: [16384,3072]
5: model.layers.0.post_attention_layernorm.weight: sum: 36.9305  dtype: BFloat16 shape: [3072]
6: model.layers.0.self_attn.o_proj.weight: sum: 2297.581  dtype: BFloat16 shape: [3072,3072]
7: model.layers.0.self_attn.qkv_proj.weight: sum: 2147.3064  dtype: BFloat16 shape: [9216,3072]
8: model.layers.1.input_layernorm.weight: sum: 37.0884  dtype: BFloat16 shape: [3072]
9: model.layers.1.mlp.down_proj.weight: sum: -3182.4885  dtype: BFloat16 shape: [3072,8192]
10: model.layers.1.mlp.gate_up_proj.weight: sum: 4781.044  dtype: BFloat16 shape: [16384,3072]
11: model.layers.1.post_attention_layernorm.weight: sum: 36.938  dtype: BFloat16 shape: [3072]
12: model.layers.1.self_attn.o_proj.weight: sum: -1783.076  dtype: BFloat16 shape: [3072,3072]
13: model.layers.1.self_attn.qkv_proj.weight: sum: 4034.25  dtype: BFloat16 shape: [9216,3072]
14: model.layers.10.input_layernorm.weight: sum: 36.9135  dtype: BFloat16 shape: [3072]
15: model.layers.10.mlp.down_proj.weight: sum: 2775.9805  dtype: BFloat16 shape: [3072,8192]
16: model.layers.10.mlp.gate_up_proj.weight: sum: 9910.286  dtype: BFloat16 shape: [16384,3072]
17: model.layers.10.post_attention_layernorm.weight: sum: 36.9509  dtype: BFloat16 shape: [3072]
18: model.layers.10.self_attn.o_proj.weight: sum: 1594.3357  dtype: BFloat16 shape: [3072,3072]
19: model.layers.10.self_attn.qkv_proj.weight: sum: 3894.8037  dtype: BFloat16 shape: [9216,3072]
20: model.layers.11.input_layernorm.weight: sum: 37.0497  dtype: BFloat16 shape: [3072]
21: model.layers.11.mlp.down_proj.weight: sum: 3097.908  dtype: BFloat16 shape: [3072,8192]
22: model.layers.11.mlp.gate_up_proj.weight: sum: 1646.3  dtype: BFloat16 shape: [16384,3072]
23: model.layers.11.post_attention_layernorm.weight: sum: 36.9316  dtype: BFloat16 shape: [3072]
24: model.layers.11.self_attn.o_proj.weight: sum: 2579.0715  dtype: BFloat16 shape: [3072,3072]
25: model.layers.11.self_attn.qkv_proj.weight: sum: 4453.9595  dtype: BFloat16 shape: [9216,3072]
26: model.layers.12.input_layernorm.weight: sum: 37.033  dtype: BFloat16 shape: [3072]
27: model.layers.12.mlp.down_proj.weight: sum: 3527.7273  dtype: BFloat16 shape: [3072,8192]
28: model.layers.12.mlp.gate_up_proj.weight: sum: 9025.829  dtype: BFloat16 shape: [16384,3072]
29: model.layers.12.post_attention_layernorm.weight: sum: 36.8791  dtype: BFloat16 shape: [3072]
30: model.layers.12.self_attn.o_proj.weight: sum: 2378.3503  dtype: BFloat16 shape: [3072,3072]
31: model.layers.12.self_attn.qkv_proj.weight: sum: -21961.09  dtype: BFloat16 shape: [9216,3072]
32: model.layers.13.input_layernorm.weight: sum: 37.0073  dtype: BFloat16 shape: [3072]
33: model.layers.13.mlp.down_proj.weight: sum: 4193.888  dtype: BFloat16 shape: [3072,8192]
34: model.layers.13.mlp.gate_up_proj.weight: sum: 5821.023  dtype: BFloat16 shape: [16384,3072]
35: model.layers.13.post_attention_layernorm.weight: sum: 36.8644  dtype: BFloat16 shape: [3072]
36: model.layers.13.self_attn.o_proj.weight: sum: 1408.5725  dtype: BFloat16 shape: [3072,3072]
37: model.layers.13.self_attn.qkv_proj.weight: sum: -1143.3278  dtype: BFloat16 shape: [9216,3072]
38: model.layers.14.input_layernorm.weight: sum: 36.8886  dtype: BFloat16 shape: [3072]
39: model.layers.14.mlp.down_proj.weight: sum: 52.878  dtype: BFloat16 shape: [3072,8192]
40: model.layers.14.mlp.gate_up_proj.weight: sum: 16504.023  dtype: BFloat16 shape: [16384,3072]
41: model.layers.14.post_attention_layernorm.weight: sum: 36.8554  dtype: BFloat16 shape: [3072]
42: model.layers.14.self_attn.o_proj.weight: sum: -196.5933  dtype: BFloat16 shape: [3072,3072]
43: model.layers.14.self_attn.qkv_proj.weight: sum: 1072.3207  dtype: BFloat16 shape: [9216,3072]
44: model.layers.15.input_layernorm.weight: sum: 37.0055  dtype: BFloat16 shape: [3072]
45: model.layers.15.mlp.down_proj.weight: sum: -3426.2908  dtype: BFloat16 shape: [3072,8192]
46: model.layers.15.mlp.gate_up_proj.weight: sum: 3402.9336  dtype: BFloat16 shape: [16384,3072]
47: model.layers.15.post_attention_layernorm.weight: sum: 36.9389  dtype: BFloat16 shape: [3072]
48: model.layers.15.self_attn.o_proj.weight: sum: 2599.0676  dtype: BFloat16 shape: [3072,3072]
49: model.layers.15.self_attn.qkv_proj.weight: sum: 4066.0076  dtype: BFloat16 shape: [9216,3072]
50: model.layers.16.input_layernorm.weight: sum: 36.8548  dtype: BFloat16 shape: [3072]
51: model.layers.16.mlp.down_proj.weight: sum: 4385.7466  dtype: BFloat16 shape: [3072,8192]
52: model.layers.16.mlp.gate_up_proj.weight: sum: 3867.515  dtype: BFloat16 shape: [16384,3072]
53: model.layers.16.post_attention_layernorm.weight: sum: 36.8809  dtype: BFloat16 shape: [3072]
54: model.layers.16.self_attn.o_proj.weight: sum: 1903.9626  dtype: BFloat16 shape: [3072,3072]
55: model.layers.16.self_attn.qkv_proj.weight: sum: 3895.7795  dtype: BFloat16 shape: [9216,3072]
56: model.layers.17.input_layernorm.weight: sum: 36.8935  dtype: BFloat16 shape: [3072]
57: model.layers.17.mlp.down_proj.weight: sum: 3028.2913  dtype: BFloat16 shape: [3072,8192]
58: model.layers.17.mlp.gate_up_proj.weight: sum: 3881.193  dtype: BFloat16 shape: [16384,3072]
59: model.layers.17.post_attention_layernorm.weight: sum: 37.0073  dtype: BFloat16 shape: [3072]
60: model.layers.17.self_attn.o_proj.weight: sum: 3819.118  dtype: BFloat16 shape: [3072,3072]
61: model.layers.17.self_attn.qkv_proj.weight: sum: 3900.9187  dtype: BFloat16 shape: [9216,3072]
62: model.layers.18.input_layernorm.weight: sum: 36.9123  dtype: BFloat16 shape: [3072]
63: model.layers.18.mlp.down_proj.weight: sum: 2559.1233  dtype: BFloat16 shape: [3072,8192]
64: model.layers.18.mlp.gate_up_proj.weight: sum: -2100.3384  dtype: BFloat16 shape: [16384,3072]
65: model.layers.18.post_attention_layernorm.weight: sum: 37.0055  dtype: BFloat16 shape: [3072]
66: model.layers.18.self_attn.o_proj.weight: sum: 1796.3604  dtype: BFloat16 shape: [3072,3072]
67: model.layers.18.self_attn.qkv_proj.weight: sum: 3196.7026  dtype: BFloat16 shape: [9216,3072]
68: model.layers.19.input_layernorm.weight: sum: 36.9881  dtype: BFloat16 shape: [3072]
69: model.layers.19.mlp.down_proj.weight: sum: 8913.258  dtype: BFloat16 shape: [3072,8192]
70: model.layers.19.mlp.gate_up_proj.weight: sum: -45731.773  dtype: BFloat16 shape: [16384,3072]
71: model.layers.19.post_attention_layernorm.weight: sum: 36.865  dtype: BFloat16 shape: [3072]
72: model.layers.19.self_attn.o_proj.weight: sum: 3251.995  dtype: BFloat16 shape: [3072,3072]
73: model.layers.19.self_attn.qkv_proj.weight: sum: 3825.42  dtype: BFloat16 shape: [9216,3072]
74: model.layers.2.input_layernorm.weight: sum: 37.0329  dtype: BFloat16 shape: [3072]
75: model.layers.2.mlp.down_proj.weight: sum: 22942.072  dtype: BFloat16 shape: [3072,8192]
76: model.layers.2.mlp.gate_up_proj.weight: sum: 3635.7937  dtype: BFloat16 shape: [16384,3072]
77: model.layers.2.post_attention_layernorm.weight: sum: 36.9277  dtype: BFloat16 shape: [3072]
78: model.layers.2.self_attn.o_proj.weight: sum: -28163.168  dtype: BFloat16 shape: [3072,3072]
79: model.layers.2.self_attn.qkv_proj.weight: sum: 3774.7185  dtype: BFloat16 shape: [9216,3072]
80: model.layers.20.input_layernorm.weight: sum: 36.9153  dtype: BFloat16 shape: [3072]
81: model.layers.20.mlp.down_proj.weight: sum: 547.3432  dtype: BFloat16 shape: [3072,8192]
82: model.layers.20.mlp.gate_up_proj.weight: sum: 2302.5806  dtype: BFloat16 shape: [16384,3072]
83: model.layers.20.post_attention_layernorm.weight: sum: 36.9236  dtype: BFloat16 shape: [3072]
84: model.layers.20.self_attn.o_proj.weight: sum: 769.0229  dtype: BFloat16 shape: [3072,3072]
85: model.layers.20.self_attn.qkv_proj.weight: sum: 2948.513  dtype: BFloat16 shape: [9216,3072]
86: model.layers.21.input_layernorm.weight: sum: 36.9773  dtype: BFloat16 shape: [3072]
87: model.layers.21.mlp.down_proj.weight: sum: 3291.6553  dtype: BFloat16 shape: [3072,8192]
88: model.layers.21.mlp.gate_up_proj.weight: sum: -2569.517  dtype: BFloat16 shape: [16384,3072]
89: model.layers.21.post_attention_layernorm.weight: sum: 37.0442  dtype: BFloat16 shape: [3072]
90: model.layers.21.self_attn.o_proj.weight: sum: 1850.041  dtype: BFloat16 shape: [3072,3072]
91: model.layers.21.self_attn.qkv_proj.weight: sum: 1948.1062  dtype: BFloat16 shape: [9216,3072]
92: model.layers.22.input_layernorm.weight: sum: 36.9873  dtype: BFloat16 shape: [3072]
93: model.layers.22.mlp.down_proj.weight: sum: 3963.3667  dtype: BFloat16 shape: [3072,8192]
94: model.layers.22.mlp.gate_up_proj.weight: sum: 8245.719  dtype: BFloat16 shape: [16384,3072]
95: model.layers.22.post_attention_layernorm.weight: sum: 36.8943  dtype: BFloat16 shape: [3072]
96: model.layers.22.self_attn.o_proj.weight: sum: 2346.5112  dtype: BFloat16 shape: [3072,3072]
97: model.layers.22.self_attn.qkv_proj.weight: sum: 3865.0056  dtype: BFloat16 shape: [9216,3072]
98: model.layers.23.input_layernorm.weight: sum: 36.8832  dtype: BFloat16 shape: [3072]
99: model.layers.23.mlp.down_proj.weight: sum: 3418.482  dtype: BFloat16 shape: [3072,8192]
100: model.layers.23.mlp.gate_up_proj.weight: sum: 7335.5254  dtype: BFloat16 shape: [16384,3072]
101: model.layers.23.post_attention_layernorm.weight: sum: 36.9405  dtype: BFloat16 shape: [3072]
102: model.layers.23.self_attn.o_proj.weight: sum: 3530.745  dtype: BFloat16 shape: [3072,3072]
103: model.layers.23.self_attn.qkv_proj.weight: sum: 6659.614  dtype: BFloat16 shape: [9216,3072]
104: model.layers.24.input_layernorm.weight: sum: 36.9235  dtype: BFloat16 shape: [3072]
105: model.layers.24.mlp.down_proj.weight: sum: 2752.4482  dtype: BFloat16 shape: [3072,8192]
106: model.layers.24.mlp.gate_up_proj.weight: sum: 4617.6313  dtype: BFloat16 shape: [16384,3072]
107: model.layers.24.post_attention_layernorm.weight: sum: 36.9215  dtype: BFloat16 shape: [3072]
108: model.layers.24.self_attn.o_proj.weight: sum: -1382.909  dtype: BFloat16 shape: [3072,3072]
109: model.layers.24.self_attn.qkv_proj.weight: sum: -616.814  dtype: BFloat16 shape: [9216,3072]
110: model.layers.25.input_layernorm.weight: sum: 36.9328  dtype: BFloat16 shape: [3072]
111: model.layers.25.mlp.down_proj.weight: sum: 3362.1597  dtype: BFloat16 shape: [3072,8192]
112: model.layers.25.mlp.gate_up_proj.weight: sum: -6831.4746  dtype: BFloat16 shape: [16384,3072]
113: model.layers.25.post_attention_layernorm.weight: sum: 36.8519  dtype: BFloat16 shape: [3072]
114: model.layers.25.self_attn.o_proj.weight: sum: 2446.9336  dtype: BFloat16 shape: [3072,3072]
115: model.layers.25.self_attn.qkv_proj.weight: sum: 2812.9602  dtype: BFloat16 shape: [9216,3072]
116: model.layers.26.input_layernorm.weight: sum: 36.9606  dtype: BFloat16 shape: [3072]
117: model.layers.26.mlp.down_proj.weight: sum: 3907.119  dtype: BFloat16 shape: [3072,8192]
118: model.layers.26.mlp.gate_up_proj.weight: sum: 3148.3582  dtype: BFloat16 shape: [16384,3072]
119: model.layers.26.post_attention_layernorm.weight: sum: 36.98  dtype: BFloat16 shape: [3072]
120: model.layers.26.self_attn.o_proj.weight: sum: 418.8951  dtype: BFloat16 shape: [3072,3072]
121: model.layers.26.self_attn.qkv_proj.weight: sum: -1950.7532  dtype: BFloat16 shape: [9216,3072]
122: model.layers.27.input_layernorm.weight: sum: 36.9514  dtype: BFloat16 shape: [3072]
123: model.layers.27.mlp.down_proj.weight: sum: 3933.161  dtype: BFloat16 shape: [3072,8192]
124: model.layers.27.mlp.gate_up_proj.weight: sum: 4785.734  dtype: BFloat16 shape: [16384,3072]
125: model.layers.27.post_attention_layernorm.weight: sum: 36.9563  dtype: BFloat16 shape: [3072]
126: model.layers.27.self_attn.o_proj.weight: sum: 1359.8348  dtype: BFloat16 shape: [3072,3072]
127: model.layers.27.self_attn.qkv_proj.weight: sum: 1944.1702  dtype: BFloat16 shape: [9216,3072]
128: model.layers.28.input_layernorm.weight: sum: 37.0515  dtype: BFloat16 shape: [3072]
129: model.layers.28.mlp.down_proj.weight: sum: 291.8123  dtype: BFloat16 shape: [3072,8192]
130: model.layers.28.mlp.gate_up_proj.weight: sum: 3515.8655  dtype: BFloat16 shape: [16384,3072]
131: model.layers.28.post_attention_layernorm.weight: sum: 36.9692  dtype: BFloat16 shape: [3072]
132: model.layers.28.self_attn.o_proj.weight: sum: 2287.1267  dtype: BFloat16 shape: [3072,3072]
133: model.layers.28.self_attn.qkv_proj.weight: sum: 5399.3613  dtype: BFloat16 shape: [9216,3072]
134: model.layers.29.input_layernorm.weight: sum: 36.8573  dtype: BFloat16 shape: [3072]
135: model.layers.29.mlp.down_proj.weight: sum: 4341.6104  dtype: BFloat16 shape: [3072,8192]
136: model.layers.29.mlp.gate_up_proj.weight: sum: 3247.1929  dtype: BFloat16 shape: [16384,3072]
137: model.layers.29.post_attention_layernorm.weight: sum: 36.9392  dtype: BFloat16 shape: [3072]
138: model.layers.29.self_attn.o_proj.weight: sum: 1325.6276  dtype: BFloat16 shape: [3072,3072]
139: model.layers.29.self_attn.qkv_proj.weight: sum: 4455.661  dtype: BFloat16 shape: [9216,3072]
140: model.layers.3.input_layernorm.weight: sum: 36.8635  dtype: BFloat16 shape: [3072]
141: model.layers.3.mlp.down_proj.weight: sum: 6494.6655  dtype: BFloat16 shape: [3072,8192]
142: model.layers.3.mlp.gate_up_proj.weight: sum: 2043.6003  dtype: BFloat16 shape: [16384,3072]
143: model.layers.3.post_attention_layernorm.weight: sum: 37.0175  dtype: BFloat16 shape: [3072]
144: model.layers.3.self_attn.o_proj.weight: sum: 2757.101  dtype: BFloat16 shape: [3072,3072]
145: model.layers.3.self_attn.qkv_proj.weight: sum: 3945.7988  dtype: BFloat16 shape: [9216,3072]
146: model.layers.30.input_layernorm.weight: sum: 36.866  dtype: BFloat16 shape: [3072]
147: model.layers.30.mlp.down_proj.weight: sum: 5993.239  dtype: BFloat16 shape: [3072,8192]
148: model.layers.30.mlp.gate_up_proj.weight: sum: 6224.3315  dtype: BFloat16 shape: [16384,3072]
149: model.layers.30.post_attention_layernorm.weight: sum: 36.9566  dtype: BFloat16 shape: [3072]
150: model.layers.30.self_attn.o_proj.weight: sum: 1727.3514  dtype: BFloat16 shape: [3072,3072]
151: model.layers.30.self_attn.qkv_proj.weight: sum: 4427.1963  dtype: BFloat16 shape: [9216,3072]
152: model.layers.31.input_layernorm.weight: sum: 36.875  dtype: BFloat16 shape: [3072]
153: model.layers.31.mlp.down_proj.weight: sum: 2509.3594  dtype: BFloat16 shape: [3072,8192]
154: model.layers.31.mlp.gate_up_proj.weight: sum: 30749.443  dtype: BFloat16 shape: [16384,3072]
155: model.layers.31.post_attention_layernorm.weight: sum: 36.9939  dtype: BFloat16 shape: [3072]
156: model.layers.31.self_attn.o_proj.weight: sum: 2473.9272  dtype: BFloat16 shape: [3072,3072]
157: model.layers.31.self_attn.qkv_proj.weight: sum: 6850.2983  dtype: BFloat16 shape: [9216,3072]
158: model.layers.4.input_layernorm.weight: sum: 36.9934  dtype: BFloat16 shape: [3072]
159: model.layers.4.mlp.down_proj.weight: sum: 4549.312  dtype: BFloat16 shape: [3072,8192]
160: model.layers.4.mlp.gate_up_proj.weight: sum: 3700.8025  dtype: BFloat16 shape: [16384,3072]
161: model.layers.4.post_attention_layernorm.weight: sum: 36.859  dtype: BFloat16 shape: [3072]
162: model.layers.4.self_attn.o_proj.weight: sum: 1675.5059  dtype: BFloat16 shape: [3072,3072]
163: model.layers.4.self_attn.qkv_proj.weight: sum: 4949.205  dtype: BFloat16 shape: [9216,3072]
164: model.layers.5.input_layernorm.weight: sum: 36.8906  dtype: BFloat16 shape: [3072]
165: model.layers.5.mlp.down_proj.weight: sum: 903.3309  dtype: BFloat16 shape: [3072,8192]
166: model.layers.5.mlp.gate_up_proj.weight: sum: 3111.4333  dtype: BFloat16 shape: [16384,3072]
167: model.layers.5.post_attention_layernorm.weight: sum: 36.987  dtype: BFloat16 shape: [3072]
168: model.layers.5.self_attn.o_proj.weight: sum: 2654.925  dtype: BFloat16 shape: [3072,3072]
169: model.layers.5.self_attn.qkv_proj.weight: sum: 3260.8071  dtype: BFloat16 shape: [9216,3072]
170: model.layers.6.input_layernorm.weight: sum: 36.8537  dtype: BFloat16 shape: [3072]
171: model.layers.6.mlp.down_proj.weight: sum: 2860.5618  dtype: BFloat16 shape: [3072,8192]
172: model.layers.6.mlp.gate_up_proj.weight: sum: -12.5355  dtype: BFloat16 shape: [16384,3072]
173: model.layers.6.post_attention_layernorm.weight: sum: 36.9677  dtype: BFloat16 shape: [3072]
174: model.layers.6.self_attn.o_proj.weight: sum: 718.7711  dtype: BFloat16 shape: [3072,3072]
175: model.layers.6.self_attn.qkv_proj.weight: sum: -2342.6191  dtype: BFloat16 shape: [9216,3072]
176: model.layers.7.input_layernorm.weight: sum: 36.8134  dtype: BFloat16 shape: [3072]
177: model.layers.7.mlp.down_proj.weight: sum: 4019.3113  dtype: BFloat16 shape: [3072,8192]
178: model.layers.7.mlp.gate_up_proj.weight: sum: 3436.098  dtype: BFloat16 shape: [16384,3072]
179: model.layers.7.post_attention_layernorm.weight: sum: 36.8465  dtype: BFloat16 shape: [3072]
180: model.layers.7.self_attn.o_proj.weight: sum: 2601.0222  dtype: BFloat16 shape: [3072,3072]
181: model.layers.7.self_attn.qkv_proj.weight: sum: 2374.7678  dtype: BFloat16 shape: [9216,3072]
182: model.layers.8.input_layernorm.weight: sum: 36.9748  dtype: BFloat16 shape: [3072]
183: model.layers.8.mlp.down_proj.weight: sum: 5026.0894  dtype: BFloat16 shape: [3072,8192]
184: model.layers.8.mlp.gate_up_proj.weight: sum: 3374.8013  dtype: BFloat16 shape: [16384,3072]
185: model.layers.8.post_attention_layernorm.weight: sum: 36.9214  dtype: BFloat16 shape: [3072]
186: model.layers.8.self_attn.o_proj.weight: sum: 2527.0618  dtype: BFloat16 shape: [3072,3072]
187: model.layers.8.self_attn.qkv_proj.weight: sum: 3104.521  dtype: BFloat16 shape: [9216,3072]
188: model.layers.9.input_layernorm.weight: sum: 37.0721  dtype: BFloat16 shape: [3072]
189: model.layers.9.mlp.down_proj.weight: sum: 54996.57  dtype: BFloat16 shape: [3072,8192]
190: model.layers.9.mlp.gate_up_proj.weight: sum: 3444.628  dtype: BFloat16 shape: [16384,3072]
191: model.layers.9.post_attention_layernorm.weight: sum: 36.8925  dtype: BFloat16 shape: [3072]
192: model.layers.9.self_attn.o_proj.weight: sum: 897.3518  dtype: BFloat16 shape: [3072,3072]
193: model.layers.9.self_attn.qkv_proj.weight: sum: -2661.9934  dtype: BFloat16 shape: [9216,3072]
194: model.norm.weight: sum: 36.9282  dtype: BFloat16 shape: [3072]
